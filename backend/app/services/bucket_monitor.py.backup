"""
Bucket monitoring service for continuous document ingestion.
Automatically processes files uploaded to S3 bucket's auto_ingest folder.
"""
import time
import threading
import logging
import asyncio
import json
from typing import Dict, Set, Optional
from datetime import datetime

from app.services.storage import S3Handler
from app.services.document import DocumentProcessor
from app.services.vector_store import VectorStore
from app.services.metrics import TTFBMonitor

logger = logging.getLogger(__name__)


class BucketMonitor:
    """Monitor S3 bucket for new files and process them automatically."""
    
    def __init__(
        self,
        vector_store: VectorStore,
        document_processor: DocumentProcessor,
        ttfb_monitor: TTFBMonitor
    ):
        self.vector_store = vector_store
        self.document_processor = document_processor
        self.ttfb_monitor = ttfb_monitor
        
        self.monitoring = False
        self.bucket_name: Optional[str] = None
        self.processed_files: Set[str] = set()
        self.monitor_thread: Optional[threading.Thread] = None
        self.poll_interval = 5  # seconds
        
        # Real-time streaming support
        self.processing_events: asyncio.Queue = asyncio.Queue(maxsize=100)
        self.current_file_progress: Dict = {}
        
        logger.info("ğŸ“‚ BucketMonitor initialized")
    
    def start_monitoring(self, bucket_name: str) -> str:
        """Start monitoring the specified S3 bucket."""
        if self.monitoring:
            return f"Already monitoring bucket: {self.bucket_name}"
        
        self.bucket_name = bucket_name
        self.monitoring = True
        
        # Start monitoring thread
        self.monitor_thread = threading.Thread(target=self._poll_bucket, daemon=True)
        self.monitor_thread.start()
        
        logger.info(f"ğŸš€ Started monitoring bucket: {bucket_name}")
        return f"Started monitoring bucket: {bucket_name}"
    
    def stop_monitoring(self) -> str:
        """Stop bucket monitoring."""
        if not self.monitoring:
            return "Monitoring is not active"
        
        self.monitoring = False
        if self.monitor_thread:
            self.monitor_thread.join(timeout=2)
        
        logger.info(f"ğŸ›‘ Stopped monitoring bucket: {self.bucket_name}")
        return "Monitoring stopped"
    
    def get_status(self) -> Dict:
        """Get current monitoring status."""
        return {
            "monitoring": self.monitoring,
            "bucket_name": self.bucket_name,
            "processed_files_count": len(self.processed_files),
            "processed_files": list(self.processed_files),
            "current_file_progress": self.current_file_progress,
            "last_check": datetime.now().isoformat() if self.monitoring else None
        }
    
    def _emit_progress(self, event_data: Dict):
        """Emit progress event for real-time streaming."""
        try:
            # Non-blocking put - if queue is full, skip this event
            self.processing_events.put_nowait(event_data)
            logger.debug(f"ğŸ“¡ Emitted progress event: {event_data.get('file')} chunk {event_data.get('chunk_index')}")
        except asyncio.QueueFull:
            logger.warning("âš ï¸ Event queue full, skipping event")
    
    async def stream_events(self):
        """Generator for SSE streaming of processing events."""
        logger.info("ğŸ“¡ SSE stream started")
        try:
            while True:
                # Wait for new event with timeout
                try:
                    event = await asyncio.wait_for(self.processing_events.get(), timeout=30.0)
                    yield f"data: {json.dumps(event)}\n\n"
                except asyncio.TimeoutError:
                    # Send keepalive
                    yield f": keepalive\n\n"
        except Exception as e:
            logger.error(f"SSE stream error: {e}")
            yield f"data: {{\"error\": \"{str(e)}\"}}\n\n"
    
    def _poll_bucket(self):
        """Poll bucket for new files continuously."""
        logger.info(f"ğŸ“¡ Polling thread started for bucket: {self.bucket_name}")
        
        while self.monitoring:
            try:
                self._check_bucket_for_new_files()
                time.sleep(self.poll_interval)
            except Exception as e:
                logger.error(f"âŒ Error polling bucket: {e}")
                time.sleep(self.poll_interval * 2)  # Back off on error
    
    def _check_bucket_for_new_files(self):
        """Check bucket for new files in auto_ingest folder."""
        if not self.bucket_name:
            return
        
        try:
            logger.debug(f"ğŸ” Scanning bucket '{self.bucket_name}' auto_ingest/ folder...")
            
            handler = S3Handler('ddn_infinia')
            success, message = handler.create_client()
            if not success:
                logger.warning(f"Failed to create DDN INFINIA client: {message}")
                return
            
            # List objects in auto_ingest folder
            objects, message = handler.list_objects(prefix='auto_ingest/')
            
            logger.debug(f"   Found {len(objects)} total objects")
            logger.debug(f"   Already processed: {len(self.processed_files)} files")
            
            if not objects:
                logger.debug("   No objects found in auto_ingest/ folder")
                return
            
            # Supported file types
            SUPPORTED_EXTENSIONS = ('.pdf', '.txt', '.docx', '.doc', '.csv', '.xlsx', '.xls', '.pptx')
            
            new_files_count = 0
            for obj in objects:
                key = obj['Key']
                
                # Skip directory markers
                if key.endswith('/'):
                    continue
                
                # Check if file type is supported
                file_extension = key.lower().split('.')[-1]
                if not key.lower().endswith(SUPPORTED_EXTENSIONS):
                    logger.debug(f"â­ï¸  Skipping unsupported file: {key}")
                    continue
                
                # Skip if already processed
                if key in self.processed_files:
                    logger.debug(f"â­ï¸  Already processed: {key}")
                    continue
                
                logger.info(f"âœ… Found new file for processing: {key}")
                new_files_count += 1
                self._process_bucket_file(key, handler)
            
            if new_files_count == 0:
                logger.debug(f"   No new files to process")
                
        except Exception as e:
            logger.error(f"âŒ Error checking bucket: {e}", exc_info=True)
    
    def _add_chunks_with_progress(self, chunks: list, filename: str, s3_key: str) -> int:
        """Add chunks one-by-one with real-time progress events."""
        total_chunks = len(chunks)
        chunks_added = 0
        
        for i, chunk in enumerate(chunks):
            chunk_start_time = time.perf_counter()
            result = self.vector_store.add_chunks([chunk])
            chunk_elapsed = time.perf_counter() - chunk_start_time
            
            if result.get('stored_chunks', 0) > 0:
                chunks_added += 1
            
            # Emit real-time progress
            self._emit_progress({
                'file': filename,
                's3_key': s3_key,
                'chunk_index': i + 1,
                'total_chunks': total_chunks,
                'progress': ((i + 1) / total_chunks) * 100,
                'chunk_time_ms': chunk_elapsed * 1000,
                'performance': result.get('provider_performance', {}),
                'aws_simulated': result.get('aws_simulated', False),
                'timestamp': datetime.now().isoformat()
            })
        
        return chunks_added
    
    def _process_bucket_file(self, s3_key: str, handler: S3Handler):
        """Download and process a file from the bucket."""
        import tempfile
        import os
        
        filename = os.path.basename(s3_key)
        
        try:
            # Log vector store state BEFORE processing
            chunks_before = self.vector_store.total_chunks
            logger.info(f"ğŸ“Š Vector store BEFORE processing '{filename}': {chunks_before} chunks (instance ID: {id(self.vector_store)})")
            
            logger.info(f"ğŸ“¥ Downloading: {s3_key}")
            
            # Download file
            file_bytes, message = handler.download_bytes(s3_key)
            
            if not file_bytes:
                logger.error(f"âŒ Failed to download {s3_key}: {message}")
                return
            
            logger.info(f"ğŸ“¥ Downloaded {len(file_bytes)} bytes from {s3_key}")
            
            if len(file_bytes) == 0:
                logger.error(f"âŒ Zero bytes downloaded for {s3_key}")
                return
            
            # Save to temporary file
            with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(filename)[1]) as tmp:
                tmp.write(file_bytes)
                tmp_path = tmp.name
            
            logger.debug(f"ğŸ’¾ Saved to temp file: {tmp_path}")
            
            try:
                # Process document
                logger.info(f"âš™ï¸  Processing document: {filename}")
                chunks = self.document_processor.process_file(tmp_path)
                
                # Validate chunks were created
                logger.info(f"âš™ï¸  DocumentProcessor returned {len(chunks) if chunks else 0} chunks")
                
                if not chunks:
                    logger.warning(f"âš ï¸  No chunks generated from {filename} - file may be empty or text extraction failed")
                    return
                
                # Log sample chunk content
                if chunks and len(chunks) > 0:
                    sample_content = chunks[0].get('content', '')[:100]
                    logger.info(f"ğŸ“¦ Sample chunk preview: {sample_content}...")
                    logger.debug(f"ğŸ“¦ Chunk metadata: {chunks[0].get('metadata', {})}")
                
                # Process chunks one-by-one with real-time progress
                logger.info(f"ğŸ’¾ Processing {len(chunks)} chunks individually...")
                chunks_added = self._add_chunks_with_progress(chunks, filename, s3_key)
                
                # Verify chunks were added
                chunks_after = self.vector_store.total_chunks
                chunks_added = chunks_after - chunks_before
                
                logger.info(f"ğŸ“Š Vector store AFTER processing '{filename}': {chunks_after} chunks (+{chunks_added} new)")
                
                if chunks_added == 0:
                    logger.error(f"âŒ WARNING: No chunks were actually added to the vector store for {filename}!")
                    logger.error(f"   Expected to add {len(chunks)} chunks but vector store size did not change")
                    return
                
                # Track metrics
                self.ttfb_monitor.add_storage_comparison(
                    results.get('provider_performance', {}),
                    f'auto_ingest_{filename}'
                )
                
                # Mark as processed ONLY if chunks were successfully added
                self.processed_files.add(s3_key)
                
                logger.info(f"âœ… Successfully processed {filename}: {chunks_added} chunks added to vector store")
                
                # Optionally delete from auto_ingest folder after processing
                # handler.delete_object(s3_key)
                # logger.info(f"ğŸ—‘ï¸  Deleted processed file: {s3_key}")
                
            finally:
                # Clean up temp file
                if os.path.exists(tmp_path):
                    os.unlink(tmp_path)
                    logger.debug(f"ğŸ§¹ Cleaned up temp file: {tmp_path}")
                    
        except Exception as e:
            logger.error(f"âŒ FAILED to process {s3_key}: {e}", exc_info=True)
            # Don't mark as processed if it failed - allow retry
            raise
